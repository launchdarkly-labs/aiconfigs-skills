{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Config AI Metrics - Cookbook\n",
    "\n",
    "Comprehensive tests for all code in SKILL.md and references.\n",
    "\n",
    "Prerequisites: `LAUNCHDARKLY_SDK_KEY`, `LAUNCHDARKLY_API_TOKEN`, AI Config created (see `aiconfig-create`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[OK] Loaded environment from /Users/ld_scarlett/Documents/Github/agent-skills/.env\n"
     ]
    }
   ],
   "source": [
    "%pip install launchdarkly-server-sdk launchdarkly-server-sdk-ai openai anthropic tiktoken requests python-dotenv -q\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def find_repo_root(start_path: Path = None) -> Path:\n",
    "    current = start_path or Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / '.git').exists():\n",
    "            return parent\n",
    "    return current\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "load_dotenv(repo_root / '.env')\n",
    "print(f\"[OK] Loaded environment from {repo_root / '.env'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SDK initialized: True\n",
      "[OK] Got config: enabled=True, model=gpt-4\n"
     ]
    }
   ],
   "source": [
    "# SDK initialization (see aiconfig-sdk for details)\n",
    "from ldclient import Context\n",
    "from ldai.client import LDAIClient, AICompletionConfigDefault\n",
    "import ldclient\n",
    "from ldclient.config import Config\n",
    "\n",
    "SDK_KEY = os.environ.get(\"LAUNCHDARKLY_SDK_KEY\")\n",
    "ldclient.set_config(Config(SDK_KEY))\n",
    "ld_client = ldclient.get()\n",
    "ai_client = LDAIClient(ld_client)\n",
    "\n",
    "print(f\"[OK] SDK initialized: {ld_client.is_initialized()}\")\n",
    "\n",
    "# Get config for testing tracker methods\n",
    "context = Context.builder(\"cookbook-test-user\").build()\n",
    "config = ai_client.completion_config(\n",
    "    \"content-assistant\",\n",
    "    context,\n",
    "    AICompletionConfigDefault(enabled=False),\n",
    "    {}\n",
    ")\n",
    "print(f\"[OK] Got config: enabled={config.enabled}, model={config.model.name if config.model else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Tracker Methods\n",
    "From: `SKILL.md` lines 69-82 (Quick Reference table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Got tracker: LDAIConfigTracker\n",
      "[OK] track_success()\n",
      "[OK] track_error()\n",
      "[OK] track_tokens(TokenUsage(total=30, input=10, output=20))\n",
      "[OK] track_duration(150)\n",
      "[OK] track_time_to_first_token(25)\n",
      "[OK] track_duration_of(fn) -> result\n",
      "[OK] Flushed metrics\n"
     ]
    }
   ],
   "source": [
    "from ldai.tracker import TokenUsage\n",
    "import time\n",
    "\n",
    "tracker = config.tracker\n",
    "print(f\"[OK] Got tracker: {type(tracker).__name__}\")\n",
    "\n",
    "# Test track_success() - SKILL.md line 81\n",
    "tracker.track_success()\n",
    "print(f\"[OK] track_success()\")\n",
    "\n",
    "# Test track_error() - SKILL.md line 82\n",
    "tracker.track_error()\n",
    "print(f\"[OK] track_error()\")\n",
    "\n",
    "# Test track_tokens(TokenUsage) - SKILL.md line 78\n",
    "tokens = TokenUsage(total=30, input=10, output=20)\n",
    "tracker.track_tokens(tokens)\n",
    "print(f\"[OK] track_tokens(TokenUsage(total=30, input=10, output=20))\")\n",
    "\n",
    "# Test track_duration(int) - SKILL.md line 79\n",
    "tracker.track_duration(150)\n",
    "print(f\"[OK] track_duration(150)\")\n",
    "\n",
    "# Test track_time_to_first_token(int) - SKILL.md line 80\n",
    "tracker.track_time_to_first_token(25)\n",
    "print(f\"[OK] track_time_to_first_token(25)\")\n",
    "\n",
    "# Test track_duration_of(fn) - SKILL.md line 77\n",
    "def slow_fn():\n",
    "    time.sleep(0.1)\n",
    "    return \"result\"\n",
    "\n",
    "result = tracker.track_duration_of(slow_fn)\n",
    "print(f\"[OK] track_duration_of(fn) -> {result}\")\n",
    "\n",
    "# Test track_openai_metrics(fn) - SKILL.md line 75 (tested in OpenAI section)\n",
    "# Test track_bedrock_converse_metrics(res) - SKILL.md line 76 (tested in Bedrock section)\n",
    "\n",
    "ld_client.flush()\n",
    "print(f\"[OK] Flushed metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## OpenAI Tracking\n",
    "From: `references/openai-tracking.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] call_with_tracking() defined (openai-tracking.md:13-31)\n"
     ]
    }
   ],
   "source": [
    "# From: references/openai-tracking.md lines 13-31\n",
    "import openai\n",
    "\n",
    "def call_with_tracking(config, user_prompt: str):\n",
    "    \"\"\"OpenAI call with automatic metrics tracking.\"\"\"\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    response = config.tracker.track_openai_metrics(\n",
    "        lambda: openai.chat.completions.create(\n",
    "            model=config.model.name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": config.messages[0].content},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"[OK] call_with_tracking() defined (openai-tracking.md:13-31)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] call_with_tracking_safe() defined (openai-tracking.md:35-55)\n"
     ]
    }
   ],
   "source": [
    "# From: references/openai-tracking.md lines 35-55\n",
    "def call_with_tracking_safe(config, user_prompt: str):\n",
    "    \"\"\"OpenAI call with metrics and error tracking.\"\"\"\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        response = config.tracker.track_openai_metrics(\n",
    "            lambda: openai.chat.completions.create(\n",
    "                model=config.model.name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": config.messages[0].content},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        config.tracker.track_error()\n",
    "        raise\n",
    "\n",
    "print(\"[OK] call_with_tracking_safe() defined (openai-tracking.md:35-55)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing OpenAI Tracking ===\n",
      "[OK] call_with_tracking: \"Hi\"\n",
      "[OK] call_with_tracking_safe: \"Farewell\"\n",
      "[OK] Flushed OpenAI metrics\n"
     ]
    }
   ],
   "source": [
    "# Test OpenAI tracking\n",
    "print(\"=== Testing OpenAI Tracking ===\")\n",
    "\n",
    "openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if openai_key and config.enabled:\n",
    "    openai.api_key = openai_key\n",
    "    \n",
    "    # Test basic tracking (openai-tracking.md:13-31)\n",
    "    result = call_with_tracking(config, \"Say hello in one word\")\n",
    "    if result:\n",
    "        print(f\"[OK] call_with_tracking: {result[:50]}\")\n",
    "    \n",
    "    # Test safe tracking (openai-tracking.md:35-55)\n",
    "    result = call_with_tracking_safe(config, \"Say goodbye in one word\")\n",
    "    if result:\n",
    "        print(f\"[OK] call_with_tracking_safe: {result[:50]}\")\n",
    "    \n",
    "    ld_client.flush()\n",
    "    print(\"[OK] Flushed OpenAI metrics\")\n",
    "else:\n",
    "    print(\"[INFO] OPENAI_API_KEY not set or config disabled - skipping live test\")\n",
    "    print(\"[OK] Functions defined and ready for use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Anthropic Tracking\n",
    "From: `references/anthropic-tracking.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] anthropic_call_with_tracking() defined (anthropic-tracking.md:13-45)\n"
     ]
    }
   ],
   "source": [
    "# From: references/anthropic-tracking.md lines 13-45\n",
    "from ldai.tracker import TokenUsage\n",
    "\n",
    "def anthropic_call_with_tracking(config, user_prompt: str):\n",
    "    \"\"\"Anthropic call with manual metrics tracking.\"\"\"\n",
    "    import anthropic\n",
    "    client = anthropic.Anthropic()\n",
    "\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    tracker = config.tracker\n",
    "\n",
    "    # Track duration of the call\n",
    "    response = tracker.track_duration_of(\n",
    "        lambda: client.messages.create(\n",
    "            model=config.model.name,\n",
    "            max_tokens=1024,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Manually track tokens using TokenUsage object\n",
    "    if hasattr(response, 'usage'):\n",
    "        tokens = TokenUsage(\n",
    "            total=response.usage.input_tokens + response.usage.output_tokens,\n",
    "            input=response.usage.input_tokens,\n",
    "            output=response.usage.output_tokens\n",
    "        )\n",
    "        tracker.track_tokens(tokens)\n",
    "\n",
    "    tracker.track_success()\n",
    "    return response.content[0].text\n",
    "\n",
    "print(\"[OK] anthropic_call_with_tracking() defined (anthropic-tracking.md:13-45)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] anthropic_call_with_system_prompt() defined (anthropic-tracking.md:49-76)\n"
     ]
    }
   ],
   "source": [
    "# From: references/anthropic-tracking.md lines 49-76\n",
    "def anthropic_call_with_system_prompt(config, user_prompt: str):\n",
    "    \"\"\"Anthropic call using system prompt from AI Config.\"\"\"\n",
    "    import anthropic\n",
    "    client = anthropic.Anthropic()\n",
    "\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    tracker = config.tracker\n",
    "    system_content = config.messages[0].content if config.messages else \"\"\n",
    "\n",
    "    response = tracker.track_duration_of(\n",
    "        lambda: client.messages.create(\n",
    "            model=config.model.name,\n",
    "            max_tokens=1024,\n",
    "            system=system_content,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if hasattr(response, 'usage'):\n",
    "        tokens = TokenUsage(\n",
    "            total=response.usage.input_tokens + response.usage.output_tokens,\n",
    "            input=response.usage.input_tokens,\n",
    "            output=response.usage.output_tokens\n",
    "        )\n",
    "        tracker.track_tokens(tokens)\n",
    "\n",
    "    tracker.track_success()\n",
    "    return response.content[0].text\n",
    "\n",
    "print(\"[OK] anthropic_call_with_system_prompt() defined (anthropic-tracking.md:49-76)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] anthropic_call_with_tracking_safe() defined (anthropic-tracking.md:80-110)\n"
     ]
    }
   ],
   "source": [
    "# From: references/anthropic-tracking.md lines 80-110\n",
    "def anthropic_call_with_tracking_safe(config, user_prompt: str):\n",
    "    \"\"\"Anthropic call with metrics and error tracking.\"\"\"\n",
    "    import anthropic\n",
    "    client = anthropic.Anthropic()\n",
    "\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    tracker = config.tracker\n",
    "\n",
    "    try:\n",
    "        response = tracker.track_duration_of(\n",
    "            lambda: client.messages.create(\n",
    "                model=config.model.name,\n",
    "                max_tokens=1024,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if hasattr(response, 'usage'):\n",
    "            tokens = TokenUsage(\n",
    "                total=response.usage.input_tokens + response.usage.output_tokens,\n",
    "                input=response.usage.input_tokens,\n",
    "                output=response.usage.output_tokens\n",
    "            )\n",
    "            tracker.track_tokens(tokens)\n",
    "\n",
    "        tracker.track_success()\n",
    "        return response.content[0].text\n",
    "\n",
    "    except Exception as e:\n",
    "        tracker.track_error()\n",
    "        raise\n",
    "\n",
    "print(\"[OK] anthropic_call_with_tracking_safe() defined (anthropic-tracking.md:80-110)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Anthropic Tracking ===\n",
      "[INFO] ANTHROPIC_API_KEY found - live test would require Anthropic model in config\n",
      "[OK] Functions defined and ready for use\n"
     ]
    }
   ],
   "source": [
    "# Test Anthropic tracking\n",
    "print(\"=== Testing Anthropic Tracking ===\")\n",
    "\n",
    "anthropic_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "if anthropic_key and config.enabled:\n",
    "    print(\"[INFO] ANTHROPIC_API_KEY found - live test would require Anthropic model in config\")\n",
    "    print(\"[OK] Functions defined and ready for use\")\n",
    "else:\n",
    "    print(\"[INFO] ANTHROPIC_API_KEY not set or config disabled - skipping live test\")\n",
    "    print(\"[OK] Functions defined and ready for use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bedrock Tracking\n",
    "From: `references/bedrock-tracking.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bedrock_call_with_tracking() defined (bedrock-tracking.md:12-35)\n"
     ]
    }
   ],
   "source": [
    "# From: references/bedrock-tracking.md lines 12-35\n",
    "def bedrock_call_with_tracking(config, user_prompt: str):\n",
    "    \"\"\"Bedrock Converse call with automatic metrics tracking.\"\"\"\n",
    "    import boto3\n",
    "    bedrock = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    tracker = config.tracker\n",
    "\n",
    "    # Make the Bedrock call\n",
    "    response = tracker.track_duration_of(\n",
    "        lambda: bedrock.converse(\n",
    "            modelId=config.model.name,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Track Bedrock-specific metrics from response\n",
    "    tracker.track_bedrock_converse_metrics(response)\n",
    "\n",
    "    return response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "print(\"[OK] bedrock_call_with_tracking() defined (bedrock-tracking.md:12-35)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bedrock_call_with_system_prompt() defined (bedrock-tracking.md:39-58)\n"
     ]
    }
   ],
   "source": [
    "# From: references/bedrock-tracking.md lines 39-58\n",
    "def bedrock_call_with_system_prompt(config, user_prompt: str):\n",
    "    \"\"\"Bedrock call using system prompt from AI Config.\"\"\"\n",
    "    import boto3\n",
    "    bedrock = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    tracker = config.tracker\n",
    "    system_content = config.messages[0].content if config.messages else \"\"\n",
    "\n",
    "    response = tracker.track_duration_of(\n",
    "        lambda: bedrock.converse(\n",
    "            modelId=config.model.name,\n",
    "            system=[{\"text\": system_content}],\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tracker.track_bedrock_converse_metrics(response)\n",
    "\n",
    "    return response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "print(\"[OK] bedrock_call_with_system_prompt() defined (bedrock-tracking.md:39-58)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bedrock_call_with_tracking_safe() defined (bedrock-tracking.md:62-83)\n"
     ]
    }
   ],
   "source": [
    "# From: references/bedrock-tracking.md lines 62-83\n",
    "def bedrock_call_with_tracking_safe(config, user_prompt: str):\n",
    "    \"\"\"Bedrock call with metrics and error tracking.\"\"\"\n",
    "    import boto3\n",
    "    bedrock = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    tracker = config.tracker\n",
    "\n",
    "    try:\n",
    "        response = tracker.track_duration_of(\n",
    "            lambda: bedrock.converse(\n",
    "                modelId=config.model.name,\n",
    "                messages=[{\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        tracker.track_bedrock_converse_metrics(response)\n",
    "        return response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        tracker.track_error()\n",
    "        raise\n",
    "\n",
    "print(\"[OK] bedrock_call_with_tracking_safe() defined (bedrock-tracking.md:62-83)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Bedrock Tracking ===\n",
      "[INFO] boto3 available - live test would require AWS credentials and Bedrock model in config\n",
      "[OK] Functions defined and ready for use\n"
     ]
    }
   ],
   "source": [
    "# Test Bedrock tracking\n",
    "print(\"=== Testing Bedrock Tracking ===\")\n",
    "\n",
    "try:\n",
    "    import boto3\n",
    "    print(\"[INFO] boto3 available - live test would require AWS credentials and Bedrock model in config\")\n",
    "    print(\"[OK] Functions defined and ready for use\")\n",
    "except ImportError:\n",
    "    print(\"[INFO] boto3 not installed - skipping Bedrock tests\")\n",
    "    print(\"[OK] Functions defined and ready for use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Tracking\n",
    "From: `references/streaming-tracking.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] call_streaming_with_tracking() defined (streaming-tracking.md:14-59)\n"
     ]
    }
   ],
   "source": [
    "# From: references/streaming-tracking.md lines 14-59\n",
    "import time\n",
    "import openai\n",
    "from ldai.tracker import TokenUsage\n",
    "\n",
    "def call_streaming_with_tracking(config, user_prompt: str):\n",
    "    \"\"\"Streaming call with TTFT and duration tracking.\"\"\"\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    tracker = config.tracker\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=config.model.name,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response_text = \"\"\n",
    "    for chunk in stream:\n",
    "        if first_token_time is None and chunk.choices[0].delta.content:\n",
    "            first_token_time = time.time()\n",
    "            ttft_ms = int((first_token_time - start_time) * 1000)\n",
    "            tracker.track_time_to_first_token(ttft_ms)\n",
    "\n",
    "        if chunk.choices[0].delta.content:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "\n",
    "    # Track final metrics (milliseconds)\n",
    "    duration_ms = int((time.time() - start_time) * 1000)\n",
    "    tracker.track_duration(duration_ms)\n",
    "    tracker.track_success()\n",
    "\n",
    "    # Estimate tokens (or use tiktoken for accuracy)\n",
    "    estimated_input = len(user_prompt.split()) * 2\n",
    "    estimated_output = len(response_text.split()) * 2\n",
    "    tokens = TokenUsage(\n",
    "        total=estimated_input + estimated_output,\n",
    "        input=estimated_input,\n",
    "        output=estimated_output\n",
    "    )\n",
    "    tracker.track_tokens(tokens)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "print(\"[OK] call_streaming_with_tracking() defined (streaming-tracking.md:14-59)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] call_streaming_accurate_tokens() defined (streaming-tracking.md:63-115)\n"
     ]
    }
   ],
   "source": [
    "# From: references/streaming-tracking.md lines 63-115\n",
    "import tiktoken\n",
    "\n",
    "def call_streaming_accurate_tokens(config, user_prompt: str):\n",
    "    \"\"\"Streaming with accurate token counting using tiktoken.\"\"\"\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    tracker = config.tracker\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "\n",
    "    # Get encoder for the model\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(config.model.name)\n",
    "    except KeyError:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=config.model.name,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response_text = \"\"\n",
    "    for chunk in stream:\n",
    "        if first_token_time is None and chunk.choices[0].delta.content:\n",
    "            first_token_time = time.time()\n",
    "            ttft_ms = int((first_token_time - start_time) * 1000)\n",
    "            tracker.track_time_to_first_token(ttft_ms)\n",
    "\n",
    "        if chunk.choices[0].delta.content:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "\n",
    "    # Track final metrics\n",
    "    duration_ms = int((time.time() - start_time) * 1000)\n",
    "    tracker.track_duration(duration_ms)\n",
    "    tracker.track_success()\n",
    "\n",
    "    # Accurate token counts\n",
    "    input_tokens = len(enc.encode(user_prompt))\n",
    "    output_tokens = len(enc.encode(response_text))\n",
    "    tokens = TokenUsage(\n",
    "        total=input_tokens + output_tokens,\n",
    "        input=input_tokens,\n",
    "        output=output_tokens\n",
    "    )\n",
    "    tracker.track_tokens(tokens)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "print(\"[OK] call_streaming_accurate_tokens() defined (streaming-tracking.md:63-115)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] call_streaming_safe() defined (streaming-tracking.md:119-154)\n"
     ]
    }
   ],
   "source": [
    "# From: references/streaming-tracking.md lines 119-154\n",
    "def call_streaming_safe(config, user_prompt: str):\n",
    "    \"\"\"Streaming call with error tracking.\"\"\"\n",
    "    if not config.enabled:\n",
    "        return None\n",
    "\n",
    "    tracker = config.tracker\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "\n",
    "    try:\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=config.model.name,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        response_text = \"\"\n",
    "        for chunk in stream:\n",
    "            if first_token_time is None and chunk.choices[0].delta.content:\n",
    "                first_token_time = time.time()\n",
    "                ttft_ms = int((first_token_time - start_time) * 1000)\n",
    "                tracker.track_time_to_first_token(ttft_ms)\n",
    "\n",
    "            if chunk.choices[0].delta.content:\n",
    "                response_text += chunk.choices[0].delta.content\n",
    "\n",
    "        duration_ms = int((time.time() - start_time) * 1000)\n",
    "        tracker.track_duration(duration_ms)\n",
    "        tracker.track_success()\n",
    "\n",
    "        return response_text\n",
    "\n",
    "    except Exception as e:\n",
    "        tracker.track_error()\n",
    "        raise\n",
    "\n",
    "print(\"[OK] call_streaming_safe() defined (streaming-tracking.md:119-154)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Streaming Tracking ===\n",
      "[OK] call_streaming_with_tracking: Hello\n",
      "[OK] call_streaming_accurate_tokens: Goodbye\n",
      "[OK] call_streaming_safe: Thanks!\n",
      "[OK] Flushed streaming metrics\n"
     ]
    }
   ],
   "source": [
    "# Test streaming tracking\n",
    "print(\"=== Testing Streaming Tracking ===\")\n",
    "\n",
    "openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if openai_key and config.enabled:\n",
    "    openai.api_key = openai_key\n",
    "    \n",
    "    # Test basic streaming (streaming-tracking.md:14-59)\n",
    "    result = call_streaming_with_tracking(config, \"Say hello in one word\")\n",
    "    if result:\n",
    "        print(f\"[OK] call_streaming_with_tracking: {result[:50]}\")\n",
    "    \n",
    "    # Test accurate token streaming (streaming-tracking.md:63-115)\n",
    "    result = call_streaming_accurate_tokens(config, \"Say goodbye in one word\")\n",
    "    if result:\n",
    "        print(f\"[OK] call_streaming_accurate_tokens: {result[:50]}\")\n",
    "    \n",
    "    # Test safe streaming (streaming-tracking.md:119-154)\n",
    "    result = call_streaming_safe(config, \"Say thanks in one word\")\n",
    "    if result:\n",
    "        print(f\"[OK] call_streaming_safe: {result[:50]}\")\n",
    "    \n",
    "    ld_client.flush()\n",
    "    print(\"[OK] Flushed streaming metrics\")\n",
    "else:\n",
    "    print(\"[INFO] OPENAI_API_KEY not set or config disabled - skipping live test\")\n",
    "    print(\"[OK] Functions defined and ready for use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metrics API\n",
    "From: `references/metrics-api.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] get_ai_config_metrics() defined (metrics-api.md:24-69)\n"
     ]
    }
   ],
   "source": [
    "# From: references/metrics-api.md lines 24-69\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "def get_ai_config_metrics(project_key: str, config_key: str, env: str = \"production\", hours: int = 24):\n",
    "    \"\"\"Get AI Config metrics for the last N hours.\"\"\"\n",
    "    API_TOKEN = os.environ.get(\"LAUNCHDARKLY_API_TOKEN\")\n",
    "\n",
    "    now = int(time.time())\n",
    "    start = now - (hours * 3600)\n",
    "\n",
    "    url = f\"https://app.launchdarkly.com/api/v2/projects/{project_key}/ai-configs/{config_key}/metrics\"\n",
    "\n",
    "    params = {\n",
    "        \"from\": start,\n",
    "        \"to\": now,\n",
    "        \"env\": env\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": API_TOKEN,\n",
    "        \"LD-API-Version\": \"beta\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        metrics = response.json()\n",
    "        print(f\"[OK] Metrics for {config_key} (last {hours} hours, {env}):\")\n",
    "        print(f\"     Generations: {metrics.get('generationCount', 0):,}\")\n",
    "        print(f\"     Success: {metrics.get('generationSuccessCount', 0):,}\")\n",
    "        print(f\"     Errors: {metrics.get('generationErrorCount', 0):,}\")\n",
    "        print(f\"     Input Tokens: {metrics.get('inputTokens', 0):,}\")\n",
    "        print(f\"     Output Tokens: {metrics.get('outputTokens', 0):,}\")\n",
    "        print(f\"     Total Tokens: {metrics.get('totalTokens', 0):,}\")\n",
    "        print(f\"     Input Cost: ${metrics.get('inputCost', 0):.4f}\")\n",
    "        print(f\"     Output Cost: ${metrics.get('outputCost', 0):.4f}\")\n",
    "        print(f\"     Duration (ms): {metrics.get('durationMs', 0):,}\")\n",
    "        print(f\"     TTFT (ms): {metrics.get('timeToFirstTokenMs', 0):,}\")\n",
    "        print(f\"     Thumbs Up: {metrics.get('thumbsUp', 0)}\")\n",
    "        print(f\"     Thumbs Down: {metrics.get('thumbsDown', 0)}\")\n",
    "        return metrics\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to get metrics: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "print(\"[OK] get_ai_config_metrics() defined (metrics-api.md:24-69)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Metrics API ===\n",
      "[OK] Metrics for content-assistant (last 24 hours, production):\n",
      "     Generations: 0\n",
      "     Success: 0\n",
      "     Errors: 0\n",
      "     Input Tokens: 0\n",
      "     Output Tokens: 0\n",
      "     Total Tokens: 0\n",
      "     Input Cost: $0.0000\n",
      "     Output Cost: $0.0000\n",
      "     Duration (ms): 0\n",
      "     TTFT (ms): 0\n",
      "     Thumbs Up: 0\n",
      "     Thumbs Down: 0\n",
      "[OK] Metrics retrieved successfully\n"
     ]
    }
   ],
   "source": [
    "# Test metrics API\n",
    "print(\"=== Testing Metrics API ===\")\n",
    "\n",
    "api_token = os.environ.get(\"LAUNCHDARKLY_API_TOKEN\")\n",
    "if api_token:\n",
    "    # Test get_ai_config_metrics (metrics-api.md:24-69)\n",
    "    metrics = get_ai_config_metrics(\"support-ai\", \"content-assistant\", hours=24)\n",
    "    if metrics:\n",
    "        print(\"[OK] Metrics retrieved successfully\")\n",
    "    else:\n",
    "        print(\"[INFO] No metrics available (endpoint may not be active yet)\")\n",
    "else:\n",
    "    print(\"[INFO] LAUNCHDARKLY_API_TOKEN not set - skipping API test\")\n",
    "    print(\"[OK] Function defined and ready for use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SDK client closed\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "ld_client.close()\n",
    "print(\"[OK] SDK client closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
