{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Config Online Evals - Cookbook\n",
    "\n",
    "Prerequisites: `LAUNCHDARKLY_SDK_KEY`, `OPENAI_API_KEY`, judges enabled via LaunchDarkly UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[OK] Loaded environment from /Users/ld_scarlett/Documents/Github/agent-skills/.env\n"
     ]
    }
   ],
   "source": [
    "%pip install launchdarkly-server-sdk launchdarkly-server-sdk-ai launchdarkly-server-sdk-ai-openai openai python-dotenv -q\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def find_repo_root(start_path: Path = None) -> Path:\n",
    "    current = start_path or Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / '.git').exists():\n",
    "            return parent\n",
    "    return current\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "load_dotenv(repo_root / '.env')\n",
    "print(f\"[OK] Loaded environment from {repo_root / '.env'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SDK initialized: True\n"
     ]
    }
   ],
   "source": [
    "# SDK initialization (see aiconfig-sdk for details)\n",
    "from ldclient import Context\n",
    "from ldclient.config import Config\n",
    "from ldai.client import LDAIClient, AICompletionConfigDefault\n",
    "import ldclient\n",
    "\n",
    "SDK_KEY = os.environ.get(\"LAUNCHDARKLY_SDK_KEY\")\n",
    "ldclient.set_config(Config(SDK_KEY))\n",
    "ld_client = ldclient.get()\n",
    "ai_client = LDAIClient(ld_client)\n",
    "print(f\"[OK] SDK initialized: {ld_client.is_initialized()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SDK: Check Judge Configuration\n",
    "From: `SKILL.md` lines 29-60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] check_judges() defined\n"
     ]
    }
   ],
   "source": [
    "def check_judges(ai_client, config_key: str, user_id: str):\n",
    "    \"\"\"Check which judges are attached to a config.\"\"\"\n",
    "    context = Context.builder(user_id).build()\n",
    "    config = ai_client.completion_config(\n",
    "        config_key,\n",
    "        context,\n",
    "        AICompletionConfigDefault(enabled=False),\n",
    "        {}\n",
    "    )\n",
    "\n",
    "    if config.judge_configuration and config.judge_configuration.judges:\n",
    "        print(\"[OK] Judges attached:\")\n",
    "        for judge in config.judge_configuration.judges:\n",
    "            print(f\"     - {judge.key}: {int(judge.sampling_rate * 100)}% sampling\")\n",
    "    else:\n",
    "        print(\"[INFO] No judges configured\")\n",
    "\n",
    "    return config.judge_configuration\n",
    "\n",
    "print(\"[OK] check_judges() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing check_judges ===\n",
      "[OK] Judges attached:\n",
      "     - ld-ai-judge-accuracy-1770164301356: 100% sampling\n",
      "     - ld-ai-judge-relevance-1770164301550: 100% sampling\n",
      "     - ld-ai-judge-toxicity-1770164301695: 100% sampling\n"
     ]
    }
   ],
   "source": [
    "# Test check_judges\n",
    "print(\"=== Testing check_judges ===\")\n",
    "judge_config = check_judges(ai_client, \"content-assistant\", \"cookbook-user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SDK: Automatic Evaluation with create_chat\n",
    "From: `SKILL.md` lines 62-97\n",
    "\n",
    "**Important:** `create_chat()` passes model parameters directly to the provider. LaunchDarkly uses `maxTokens` (camelCase) but OpenAI expects `max_tokens` (snake_case). For this test, we use a variation without `maxTokens` parameters (targeting set to `cookbook-test` variation for `cookbook-user`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] generate_with_automatic_evaluation() defined\n"
     ]
    }
   ],
   "source": [
    "# Note: asyncio import needed for notebook execution\n",
    "import asyncio\n",
    "from ldai.client import AICompletionConfigDefault, ModelConfig, ProviderConfig, LDMessage\n",
    "\n",
    "async def generate_with_automatic_evaluation(ai_client, config_key: str, user_id: str, prompt: str):\n",
    "    \"\"\"Generate AI response with automatic judge evaluation using create_chat.\"\"\"\n",
    "    context = Context.builder(user_id).build()\n",
    "\n",
    "    chat = await ai_client.create_chat(\n",
    "        config_key,\n",
    "        context,\n",
    "        AICompletionConfigDefault(\n",
    "            enabled=True,\n",
    "            model=ModelConfig(\"gpt-4\"),\n",
    "            provider=ProviderConfig(\"openai\"),\n",
    "            messages=[LDMessage(role='system', content='You are a helpful assistant.')]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if not chat:\n",
    "        return None\n",
    "\n",
    "    # Invoke chat - judges evaluate automatically (1-2 min delay)\n",
    "    response = await chat.invoke(prompt)\n",
    "\n",
    "    # Results appear in Monitoring tab as:\n",
    "    # $ld:ai:judge:accuracy, $ld:ai:judge:relevance, $ld:ai:judge:toxicity\n",
    "    return response.message.content\n",
    "\n",
    "print(\"[OK] generate_with_automatic_evaluation() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing generate_with_automatic_evaluation ===\n",
      "[OK] Response: Hello! How can I assist you today?\n",
      "...\n",
      "[OK] Events flushed - check Monitoring tab in 1-2 minutes\n"
     ]
    }
   ],
   "source": [
    "# Test generate_with_automatic_evaluation\n",
    "# NOTE: cookbook-user is targeted to cookbook-test variation (no maxTokens parameter)\n",
    "print(\"=== Testing generate_with_automatic_evaluation ===\")\n",
    "openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if openai_key:\n",
    "    result = await generate_with_automatic_evaluation(ai_client, \"content-assistant\", \"cookbook-user\", \"Say hello\")\n",
    "    if result:\n",
    "        print(f\"[OK] Response: {result[:100]}...\")\n",
    "        ld_client.flush()\n",
    "        print(\"[OK] Events flushed - check Monitoring tab in 1-2 minutes\")\n",
    "    else:\n",
    "        print(\"[INFO] Config not enabled or create_chat returned None\")\n",
    "else:\n",
    "    print(\"[INFO] OPENAI_API_KEY not set - skipping live test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Viewing Results\n",
    "\n",
    "Judge evaluation results appear in the **Monitoring tab** of your AI Config:\n",
    "\n",
    "1. Go to **AI Configs** in LaunchDarkly\n",
    "2. Select your config (`content-assistant`)\n",
    "3. Click **Monitoring** tab\n",
    "4. View judge scores: `$ld:ai:judge:accuracy`, `$ld:ai:judge:relevance`, `$ld:ai:judge:toxicity`\n",
    "\n",
    "Results take 1-2 minutes to appear after generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
