{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Config Custom Metrics - Cookbook\n",
    "\n",
    "Full lifecycle: Create, Track, Get, Update, Delete metrics.\n",
    "\n",
    "Prerequisites: `LAUNCHDARKLY_SDK_KEY`, `LAUNCHDARKLY_API_TOKEN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[OK] Loaded environment from /Users/ld_scarlett/Documents/Github/agent-skills/.env\n"
     ]
    }
   ],
   "source": [
    "%pip install launchdarkly-server-sdk python-dotenv requests -q\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def find_repo_root(start_path: Path = None) -> Path:\n",
    "    current = start_path or Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / '.git').exists():\n",
    "            return parent\n",
    "    return current\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "load_dotenv(repo_root / '.env')\n",
    "print(f\"[OK] Loaded environment from {repo_root / '.env'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SDK initialized: True\n",
      "[OK] API token configured: True\n"
     ]
    }
   ],
   "source": [
    "# SDK initialization (see aiconfig-sdk for details)\n",
    "from ldclient import Context\n",
    "from ldclient.config import Config\n",
    "import ldclient\n",
    "import requests\n",
    "\n",
    "SDK_KEY = os.environ.get(\"LAUNCHDARKLY_SDK_KEY\")\n",
    "API_TOKEN = os.environ.get(\"LAUNCHDARKLY_API_TOKEN\")\n",
    "PROJECT_KEY = \"support-ai\"\n",
    "\n",
    "ldclient.set_config(Config(SDK_KEY))\n",
    "ld_client = ldclient.get()\n",
    "print(f\"[OK] SDK initialized: {ld_client.is_initialized()}\")\n",
    "print(f\"[OK] API token configured: {bool(API_TOKEN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Create Metric (API)\n",
    "From: `SKILL.md` lines 32-78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] create_metric() defined\n"
     ]
    }
   ],
   "source": [
    "def create_metric(\n",
    "    project_key: str,\n",
    "    metric_key: str,\n",
    "    name: str,\n",
    "    kind: str = \"custom\",\n",
    "    is_numeric: bool = True,\n",
    "    unit: str = \"count\",\n",
    "    success_criteria: str = \"HigherThanBaseline\",\n",
    "    event_key: str = None,\n",
    "    description: str = None\n",
    "):\n",
    "    \"\"\"Create a new metric definition in LaunchDarkly.\"\"\"\n",
    "    API_TOKEN = os.environ.get(\"LAUNCHDARKLY_API_TOKEN\")\n",
    "\n",
    "    url = f\"https://app.launchdarkly.com/api/v2/metrics/{project_key}\"\n",
    "\n",
    "    payload = {\n",
    "        \"key\": metric_key,\n",
    "        \"name\": name,\n",
    "        \"kind\": kind,\n",
    "        \"isNumeric\": is_numeric,\n",
    "        \"eventKey\": event_key or metric_key\n",
    "    }\n",
    "\n",
    "    # Unit and successCriteria are required for numeric custom metrics\n",
    "    if is_numeric and kind == \"custom\":\n",
    "        payload[\"unit\"] = unit\n",
    "        payload[\"successCriteria\"] = success_criteria\n",
    "\n",
    "    if description:\n",
    "        payload[\"description\"] = description\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": API_TOKEN,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 201:\n",
    "        print(f\"[OK] Created metric: {metric_key}\")\n",
    "        return response.json()\n",
    "    elif response.status_code == 409:\n",
    "        print(f\"[INFO] Metric already exists: {metric_key}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to create metric: {response.status_code}\")\n",
    "        print(f\"        {response.text}\")\n",
    "        return None\n",
    "\n",
    "print(\"[OK] create_metric() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing create_metric ===\n",
      "[OK] Created metric: cookbook.test.metric\n"
     ]
    }
   ],
   "source": [
    "# Test create_metric\n",
    "print(\"=== Testing create_metric ===\")\n",
    "TEST_METRIC_KEY = \"cookbook.test.metric\"\n",
    "\n",
    "result = create_metric(\n",
    "    PROJECT_KEY,\n",
    "    TEST_METRIC_KEY,\n",
    "    name=\"Cookbook Test Metric\",\n",
    "    kind=\"custom\",\n",
    "    is_numeric=True,\n",
    "    description=\"Test metric created by cookbook\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Track Events (SDK)\n",
    "From: `SKILL.md` lines 89-165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] track_metric() defined\n"
     ]
    }
   ],
   "source": [
    "def track_metric(ld_client, user_id: str, metric_key: str, value: float, data: dict = None):\n",
    "    \"\"\"Track an event to a metric.\"\"\"\n",
    "    context = Context.builder(user_id).build()\n",
    "\n",
    "    ld_client.track(\n",
    "        metric_key,\n",
    "        context,\n",
    "        data=data,\n",
    "        metric_value=value\n",
    "    )\n",
    "\n",
    "print(\"[OK] track_metric() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing track_metric ===\n",
      "[OK] Tracked: cookbook.test.metric = 100.0\n",
      "[OK] Tracked: cookbook.test.metric = 85.0 with metadata\n",
      "[OK] Events flushed\n"
     ]
    }
   ],
   "source": [
    "# Test track_metric\n",
    "print(\"=== Testing track_metric ===\")\n",
    "track_metric(ld_client, \"cookbook-user\", TEST_METRIC_KEY, 100.0)\n",
    "print(f\"[OK] Tracked: {TEST_METRIC_KEY} = 100.0\")\n",
    "\n",
    "track_metric(ld_client, \"cookbook-user\", TEST_METRIC_KEY, 85.0, {\"source\": \"test\"})\n",
    "print(f\"[OK] Tracked: {TEST_METRIC_KEY} = 85.0 with metadata\")\n",
    "\n",
    "ld_client.flush()\n",
    "print(\"[OK] Events flushed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Common Tracking Patterns\n",
    "From: `SKILL.md` lines 112-165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Common tracking patterns defined\n"
     ]
    }
   ],
   "source": [
    "def track_conversion(ld_client, user_id: str, amount: float, config_key: str):\n",
    "    \"\"\"Track a conversion event with revenue.\"\"\"\n",
    "    context = Context.builder(user_id).build()\n",
    "\n",
    "    ld_client.track(\n",
    "        \"business.conversion\",\n",
    "        context,\n",
    "        data={\"configKey\": config_key, \"category\": \"electronics\"},\n",
    "        metric_value=amount\n",
    "    )\n",
    "\n",
    "def track_task_success(ld_client, user_id: str, task_type: str, success: bool):\n",
    "    \"\"\"Track task completion success/failure.\"\"\"\n",
    "    context = Context.builder(user_id).build()\n",
    "\n",
    "    ld_client.track(\n",
    "        \"task.success_rate\",\n",
    "        context,\n",
    "        data={\"taskType\": task_type},\n",
    "        metric_value=1.0 if success else 0.0\n",
    "    )\n",
    "\n",
    "def track_satisfaction(ld_client, user_id: str, score: float, feedback_type: str):\n",
    "    \"\"\"Track user satisfaction (0-100 scale).\"\"\"\n",
    "    context = Context.builder(user_id).build()\n",
    "\n",
    "    ld_client.track(\n",
    "        \"user.satisfaction\",\n",
    "        context,\n",
    "        data={\"feedbackType\": feedback_type},\n",
    "        metric_value=score\n",
    "    )\n",
    "\n",
    "    # Track negative feedback separately for alerts\n",
    "    if score < 50:\n",
    "        ld_client.track(\n",
    "            \"user.negative_feedback\",\n",
    "            context,\n",
    "            metric_value=1.0\n",
    "        )\n",
    "\n",
    "def track_revenue(ld_client, user_id: str, revenue: float, source: str):\n",
    "    \"\"\"Track revenue generated after AI interaction.\"\"\"\n",
    "    context = Context.builder(user_id).set(\"tier\", \"premium\").build()\n",
    "\n",
    "    if revenue > 0:\n",
    "        ld_client.track(\n",
    "            \"revenue.impact\",\n",
    "            context,\n",
    "            data={\"source\": source},\n",
    "            metric_value=revenue\n",
    "        )\n",
    "\n",
    "print(\"[OK] Common tracking patterns defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Common Patterns ===\n",
      "[OK] Tracked: business.conversion = 150.00\n",
      "[OK] Tracked: task.success_rate = 1.0\n",
      "[OK] Tracked: user.satisfaction = 85.0\n",
      "[OK] Tracked: revenue.impact = 99.99\n",
      "[OK] Events flushed\n"
     ]
    }
   ],
   "source": [
    "# Test common tracking patterns\n",
    "print(\"=== Testing Common Patterns ===\")\n",
    "\n",
    "track_conversion(ld_client, \"cookbook-user\", 150.00, \"content-assistant\")\n",
    "print(\"[OK] Tracked: business.conversion = 150.00\")\n",
    "\n",
    "track_task_success(ld_client, \"cookbook-user\", \"summarization\", True)\n",
    "print(\"[OK] Tracked: task.success_rate = 1.0\")\n",
    "\n",
    "track_satisfaction(ld_client, \"cookbook-user\", 85.0, \"thumbs_up\")\n",
    "print(\"[OK] Tracked: user.satisfaction = 85.0\")\n",
    "\n",
    "track_revenue(ld_client, \"cookbook-user\", 99.99, \"recommendation\")\n",
    "print(\"[OK] Tracked: revenue.impact = 99.99\")\n",
    "\n",
    "ld_client.flush()\n",
    "print(\"[OK] Events flushed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Get Metrics (API)\n",
    "From: `SKILL.md` lines 171-223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] get_metric() defined\n"
     ]
    }
   ],
   "source": [
    "def get_metric(project_key: str, metric_key: str):\n",
    "    \"\"\"Get a single metric definition.\"\"\"\n",
    "    API_TOKEN = os.environ.get(\"LAUNCHDARKLY_API_TOKEN\")\n",
    "\n",
    "    url = f\"https://app.launchdarkly.com/api/v2/metrics/{project_key}/{metric_key}\"\n",
    "\n",
    "    headers = {\"Authorization\": API_TOKEN}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        metric = response.json()\n",
    "        print(f\"[OK] Metric: {metric['key']}\")\n",
    "        print(f\"     Name: {metric.get('name', 'N/A')}\")\n",
    "        print(f\"     Kind: {metric.get('kind', 'N/A')}\")\n",
    "        print(f\"     Numeric: {metric.get('isNumeric', False)}\")\n",
    "        print(f\"     Event Key: {metric.get('eventKey', 'N/A')}\")\n",
    "        return metric\n",
    "    elif response.status_code == 404:\n",
    "        print(f\"[INFO] Metric not found: {metric_key}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to get metric: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "print(\"[OK] get_metric() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing get_metric ===\n",
      "[OK] Metric: cookbook.test.metric\n",
      "     Name: Cookbook Test Metric\n",
      "     Kind: custom\n",
      "     Numeric: True\n",
      "     Event Key: cookbook.test.metric\n"
     ]
    }
   ],
   "source": [
    "# Test get_metric\n",
    "print(\"=== Testing get_metric ===\")\n",
    "metric = get_metric(PROJECT_KEY, TEST_METRIC_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] list_metrics() defined\n"
     ]
    }
   ],
   "source": [
    "def list_metrics(project_key: str, limit: int = 20):\n",
    "    \"\"\"List all metrics in a project.\"\"\"\n",
    "    API_TOKEN = os.environ.get(\"LAUNCHDARKLY_API_TOKEN\")\n",
    "\n",
    "    url = f\"https://app.launchdarkly.com/api/v2/metrics/{project_key}\"\n",
    "\n",
    "    headers = {\"Authorization\": API_TOKEN}\n",
    "    params = {\"limit\": limit}\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        metrics = data.get(\"items\", [])\n",
    "        print(f\"[OK] Found {len(metrics)} metrics:\")\n",
    "        for metric in metrics:\n",
    "            numeric = \"numeric\" if metric.get(\"isNumeric\") else \"non-numeric\"\n",
    "            print(f\"     - {metric['key']} ({metric.get('kind', 'custom')}, {numeric})\")\n",
    "        return metrics\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to list metrics: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "print(\"[OK] list_metrics() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing list_metrics ===\n",
      "[OK] Found 10 metrics:\n",
      "     - ld_autogen__ai-input-tokens (custom, numeric)\n",
      "     - ld_autogen__ai-output-tokens (custom, numeric)\n",
      "     - ld_autogen__ai-total-tokens (custom, numeric)\n",
      "     - ld_autogen__ai-completion-duration (custom, numeric)\n",
      "     - ld_autogen__ai-time-to-first-token (custom, numeric)\n",
      "     - ld_autogen__ai-positive-feedback-count (custom, non-numeric)\n",
      "     - ld_autogen__ai-positive-feedback-rate (custom, non-numeric)\n",
      "     - ld_autogen__ai-negative-feedback-count (custom, non-numeric)\n",
      "     - ld_autogen__ai-negative-feedback-rate (custom, non-numeric)\n",
      "     - ld_autogen__ai-completion-success (custom, non-numeric)\n"
     ]
    }
   ],
   "source": [
    "# Test list_metrics\n",
    "print(\"=== Testing list_metrics ===\")\n",
    "metrics = list_metrics(PROJECT_KEY, limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Update Metric (API)\n",
    "From: `SKILL.md` lines 227-268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] update_metric() and rename_metric() defined\n"
     ]
    }
   ],
   "source": [
    "def update_metric(project_key: str, metric_key: str, updates: list):\n",
    "    \"\"\"\n",
    "    Update a metric using JSON Patch operations.\n",
    "\n",
    "    Args:\n",
    "        updates: List of patch operations, e.g.:\n",
    "            [{\"op\": \"replace\", \"path\": \"/name\", \"value\": \"New Name\"}]\n",
    "    \"\"\"\n",
    "    API_TOKEN = os.environ.get(\"LAUNCHDARKLY_API_TOKEN\")\n",
    "\n",
    "    url = f\"https://app.launchdarkly.com/api/v2/metrics/{project_key}/{metric_key}\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": API_TOKEN,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.patch(url, json=updates, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f\"[OK] Updated metric: {metric_key}\")\n",
    "        return response.json()\n",
    "    elif response.status_code == 404:\n",
    "        print(f\"[ERROR] Metric not found: {metric_key}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to update metric: {response.status_code}\")\n",
    "        print(f\"        {response.text}\")\n",
    "        return None\n",
    "\n",
    "def rename_metric(project_key: str, metric_key: str, new_name: str, new_description: str = None):\n",
    "    \"\"\"Rename a metric and optionally update description.\"\"\"\n",
    "    updates = [\n",
    "        {\"op\": \"replace\", \"path\": \"/name\", \"value\": new_name}\n",
    "    ]\n",
    "    if new_description:\n",
    "        updates.append({\"op\": \"replace\", \"path\": \"/description\", \"value\": new_description})\n",
    "\n",
    "    return update_metric(project_key, metric_key, updates)\n",
    "\n",
    "print(\"[OK] update_metric() and rename_metric() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing update_metric ===\n",
      "[OK] Updated metric: cookbook.test.metric\n",
      "[OK] New name: Cookbook Test Metric (Updated)\n"
     ]
    }
   ],
   "source": [
    "# Test update_metric\n",
    "print(\"=== Testing update_metric ===\")\n",
    "result = rename_metric(\n",
    "    PROJECT_KEY,\n",
    "    TEST_METRIC_KEY,\n",
    "    \"Cookbook Test Metric (Updated)\",\n",
    "    \"Updated description from cookbook\"\n",
    ")\n",
    "\n",
    "# Verify the update\n",
    "if result:\n",
    "    print(f\"[OK] New name: {result.get('name')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Delete Metric (API)\n",
    "From: `SKILL.md` lines 272-292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] delete_metric() defined\n"
     ]
    }
   ],
   "source": [
    "def delete_metric(project_key: str, metric_key: str):\n",
    "    \"\"\"Delete a metric from the project.\"\"\"\n",
    "    API_TOKEN = os.environ.get(\"LAUNCHDARKLY_API_TOKEN\")\n",
    "\n",
    "    url = f\"https://app.launchdarkly.com/api/v2/metrics/{project_key}/{metric_key}\"\n",
    "\n",
    "    headers = {\"Authorization\": API_TOKEN}\n",
    "\n",
    "    response = requests.delete(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 204:\n",
    "        print(f\"[OK] Deleted metric: {metric_key}\")\n",
    "        return True\n",
    "    elif response.status_code == 404:\n",
    "        print(f\"[INFO] Metric not found: {metric_key}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"[ERROR] Failed to delete metric: {response.status_code}\")\n",
    "        return False\n",
    "\n",
    "print(\"[OK] delete_metric() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing delete_metric ===\n",
      "[OK] Deleted metric: cookbook.test.metric\n",
      "\n",
      "=== Verifying deletion ===\n",
      "[INFO] Metric not found: cookbook.test.metric\n"
     ]
    }
   ],
   "source": [
    "# Test delete_metric - cleanup the test metric\n",
    "print(\"=== Testing delete_metric ===\")\n",
    "delete_metric(PROJECT_KEY, TEST_METRIC_KEY)\n",
    "\n",
    "# Verify deletion\n",
    "print(\"\\n=== Verifying deletion ===\")\n",
    "get_metric(PROJECT_KEY, TEST_METRIC_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Session Metrics Tracker\n",
    "From: `SKILL.md` lines 343-403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SessionMetricsTracker class defined\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class SessionMetricsTracker:\n",
    "    \"\"\"Track metrics across an entire user session.\"\"\"\n",
    "\n",
    "    def __init__(self, ld_client):\n",
    "        self.ld_client = ld_client\n",
    "        self.session_data = {}\n",
    "\n",
    "    def start_session(self, user_id: str, session_id: str):\n",
    "        \"\"\"Initialize session tracking.\"\"\"\n",
    "        self.session_data[session_id] = {\n",
    "            \"user_id\": user_id,\n",
    "            \"start_time\": time.time(),\n",
    "            \"interactions\": 0,\n",
    "            \"successful_tasks\": 0\n",
    "        }\n",
    "\n",
    "    def track_interaction(self, session_id: str, success: bool):\n",
    "        \"\"\"Track individual interaction within session.\"\"\"\n",
    "        if session_id not in self.session_data:\n",
    "            return\n",
    "        session = self.session_data[session_id]\n",
    "        session[\"interactions\"] += 1\n",
    "        if success:\n",
    "            session[\"successful_tasks\"] += 1\n",
    "\n",
    "    def end_session(self, session_id: str):\n",
    "        \"\"\"Finalize and track session metrics.\"\"\"\n",
    "        if session_id not in self.session_data:\n",
    "            return None\n",
    "\n",
    "        session = self.session_data[session_id]\n",
    "        duration = time.time() - session[\"start_time\"]\n",
    "\n",
    "        context = Context.builder(session[\"user_id\"]).build()\n",
    "\n",
    "        # Track session duration\n",
    "        self.ld_client.track(\n",
    "            \"session.duration\",\n",
    "            context,\n",
    "            data={\"interactions\": session[\"interactions\"]},\n",
    "            metric_value=duration\n",
    "        )\n",
    "\n",
    "        # Track session success rate\n",
    "        if session[\"interactions\"] > 0:\n",
    "            success_rate = session[\"successful_tasks\"] / session[\"interactions\"]\n",
    "            self.ld_client.track(\n",
    "                \"session.success_rate\",\n",
    "                context,\n",
    "                metric_value=success_rate * 100\n",
    "            )\n",
    "\n",
    "        result = dict(session)\n",
    "        result[\"duration\"] = duration\n",
    "        del self.session_data[session_id]\n",
    "        return result\n",
    "\n",
    "print(\"[OK] SessionMetricsTracker class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing SessionMetricsTracker ===\n",
      "[OK] Session: 3 interactions, 2 successful\n",
      "[OK] Success rate: 66.7%\n"
     ]
    }
   ],
   "source": [
    "# Test SessionMetricsTracker\n",
    "print(\"=== Testing SessionMetricsTracker ===\")\n",
    "tracker = SessionMetricsTracker(ld_client)\n",
    "tracker.start_session(\"cookbook-user\", \"test-session-001\")\n",
    "tracker.track_interaction(\"test-session-001\", success=True)\n",
    "tracker.track_interaction(\"test-session-001\", success=True)\n",
    "tracker.track_interaction(\"test-session-001\", success=False)\n",
    "result = tracker.end_session(\"test-session-001\")\n",
    "print(f\"[OK] Session: {result['interactions']} interactions, {result['successful_tasks']} successful\")\n",
    "print(f\"[OK] Success rate: {(result['successful_tasks']/result['interactions'])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All events flushed\n"
     ]
    }
   ],
   "source": [
    "# Final flush\n",
    "ld_client.flush()\n",
    "print(\"[OK] All events flushed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
